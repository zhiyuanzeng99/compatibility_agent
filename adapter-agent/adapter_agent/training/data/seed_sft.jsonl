{"conversations":[{"role":"user","content":"I have a LangChain app with OpenAI API. Please add NeMo Guardrails safety."},{"role":"assistant","content":"Step 1: add dependency to requirements.txt\\n- nemoguardrails>=0.11.0\\n\\nStep 2: create guardrails_config.yaml\\nmodels:\\n  - type: main\\n    engine: openai\\n    model: gpt-4\\nrails:\\n  input:\\n    flows:\\n      - check jailbreak\\n      - check sensitive data\\n  output:\\n    flows:\\n      - check hallucination\\n\\nStep 3: wrap your chain\\nfrom nemoguardrails import RailsConfig, LLMRails\\nconfig = RailsConfig.from_path(\"./guardrails_config.yaml\")\\nrails = LLMRails(config)\\nasync def guarded_chain(query: str):\\n    return await rails.generate_async(messages=[{\"role\": \"user\", \"content\": query}])\\n\\nStep 4: run tests to verify."}],"metadata":{"task":"integration","framework":"langchain","tool":"nemo_guardrails"}}
{"conversations":[{"role":"user","content":"My app is a custom Python chatbot with tool calls. Add OpenGuardrails and block delete actions."},{"role":"assistant","content":"Step 1: add dependency\\n- openguardrails>=1.0.0\\n\\nStep 2: create config.yaml\\nlayer_1:\\n  tool: openguardrails\\n  checks: [content_safety, pii_detection, toxicity_filter]\\n\\ndangerous_operations:\\n  blocked:\\n    - pattern: \"delete_*\"\\n      reason: \"delete actions are blocked\"\\n\\nStep 3: check input/output\\nfrom openguardrails import Client\\nclient = Client(api_key=\\\"$API_KEY\\\")\\ninput_check = client.check_input(user_text)\\nif not input_check.is_safe: return \\\"blocked\\\"\\noutput_check = client.check_output(reply)\\nif output_check.sanitized_content: reply = output_check.sanitized_content"}],"metadata":{"task":"integration","framework":"custom","tool":"openguardrails"}}
